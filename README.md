# LLM Reading List


### Prompt Engineering
- Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. [pdf](https://arxiv.org/pdf/2309.16797.pdf)(Google deepmind) arXiv, 2023.
  
- See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning. [pdf](https://arxiv.org/pdf/2301.05226.pdf) arXiv, 2023.
  
- Scaling Instruction-Finetuned Language Models. [pdf](https://arxiv.org/abs/2210.11416) arXiv, 2022.

- Automatic Chain of Thought Prompting in Large Language Models. [pdf](https://arxiv.org/abs/2210.03493) arXiv, 2023.

- Multimodal Chain-of-Thought Reasoning in Language Models. [pdf](https://arxiv.org/pdf/2302.00923.pdf) arXiv, 2023.

- Design of a Chain-of-Thought in Math Problem Solving. [pdf](https://arxiv.org/pdf/2309.11054.pdf) arXiv, 2023.

- Large Language Models Are Human-Level Prompt Engineers. [pdf](https://arxiv.org/pdf/2211.01910.pdf) ICLR, 2023.

- ReAct: Synergizing Reasoning and Acting in Language Models. [pdf](https://arxiv.org/pdf/2210.03629.pdf) ICLR, 2023.

- Prompting Is Programming: A Query Language for Large Language Models. [pdf](https://arxiv.org/pdf/2212.06094.pdf) PLDI 2023.

- Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs. [pdf](https://arxiv.org/pdf/2305.11792.pdf) arXiv, 2023.

### Robustness
- RARR: Researching and Revising What Language Models Say, Using Language Models. [pdf](https://arxiv.org/pdf/2210.08726.pdf) arXiv, 2023.

- Fundamental Limitations of Alignment in Large Language Models. [pdf](https://arxiv.org/pdf/2304.11082.pdf) arXiv, 2023

- DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. [pdf](https://arxiv.org/pdf/2306.11698.pdf) arXiv, 2023

### Jailbreak

- GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher. [pdf](https://arxiv.org/pdf/2308.06463.pdf) arXiv, 2023.

- Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models. [pdf](http://arxiv.org/pdf/2310.02949.pdf) arXiv, 2023.

- Visual Adversarial Examples Jailbreak Aligned Large Language Models. [pdf](https://arxiv.org/abs/2306.13213) arXiv, 2023.

- Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! [pdf](https://arxiv.org/abs/2310.03693) [website](https://llm-tuning-safety.github.io/) arXiv, 2023.

- JAILBREAKER: Automated Jailbreak Across Multiple Large Language Model Chatbots. [pdf](https://arxiv.org/pdf/2307.08715.pdf) NDSS, 2024.

- Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study. [pdf](https://arxiv.org/pdf/2305.13860.pdf) arXiv, 2023.

- Multi-step Jailbreaking Privacy Attacks on ChatGPT. [pdf](https://arxiv.org/pdf/2304.05197.pdf) arXiv, 2023.

- Jailbroken: How Does LLM Safety Training Fail? [pdf](https://arxiv.org/pdf/2307.02483.pdf) arXiv, 2023.

- [workshop] On the Privacy Risk of In-context Learning. [pdf](https://trustnlpworkshop.github.io/papers/13.pdf) arXiv, 2023.


### Others
- LAMBRETTA: Learning to Rank for Twitter Soft Moderation. [pdf](https://arxiv.org/pdf/2212.05926.pdf) S&P, 2023.

- SoK: Content Moderation in Social Media, from Guidelines to Enforcement, and Research to Practice.[pdf](https://arxiv.org/pdf/2206.14855.pdf) arXiv, 2023.

- You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content. [pdf](https://yangzhangalmo.github.io/papers/SP24-ToxicPrompt.pdf) S&P, 2024.

- Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection. [pdf](https://aclanthology.org/2023.acl-long.22.pdf) ACL 2023.

- Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning. [pdf](https://arxiv.org/pdf/2310.11397.pdf) arXiv, 2023.

- Is ChatGPT a General-Purpose Natural Language Processing Task Solver? [pdf](https://arxiv.org/abs/2302.06476) arXiv, 2023.

- Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models. [pdf](https://arxiv.org/pdf/2311.00871.pdf) arXiv, 2023.

- 
