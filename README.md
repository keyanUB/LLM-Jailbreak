# LLM Reading List


### Prompt Engineering
- Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. [pdf](https://arxiv.org/pdf/2309.16797.pdf)(Google deepmind) arXiv, 2023.
  
- See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning. [pdf](https://arxiv.org/pdf/2301.05226.pdf) arXiv, 2023.
  
- Scaling Instruction-Finetuned Language Models. [pdf](https://arxiv.org/abs/2210.11416) arXiv, 2022.

- Automatic Chain of Thought Prompting in Large Language Models. [pdf](https://arxiv.org/abs/2210.03493) arXiv, 2023.

- Multimodal Chain-of-Thought Reasoning in Language Models. [pdf](https://arxiv.org/pdf/2302.00923.pdf) arXiv, 2023.

- Design of a Chain-of-Thought in Math Problem Solving. [pdf](https://arxiv.org/pdf/2309.11054.pdf) arXiv, 2023.

- Large Language Models Are Human-Level Prompt Engineers. [pdf](https://arxiv.org/pdf/2211.01910.pdf) ICLR, 2023.

- ReAct: Synergizing Reasoning and Acting in Language Models. [pdf](https://arxiv.org/pdf/2210.03629.pdf) ICLR, 2023.

- Prompting Is Programming: A Query Language for Large Language Models. [pdf](https://arxiv.org/pdf/2212.06094.pdf) PLDI 2023.

- Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs. [pdf](https://arxiv.org/pdf/2305.11792.pdf) arXiv, 2023.
  
- Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine. [pdf](https://arxiv.org/pdf/2311.16452.pdf) arXiv, 2023.

- Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning. [pdf](https://arxiv.org/pdf/2305.14160.pdf), 2023

### Robustness and Safety Alignment
- RARR: Researching and Revising What Language Models Say, Using Language Models. [pdf](https://arxiv.org/pdf/2210.08726.pdf) arXiv, 2023.

- Fundamental Limitations of Alignment in Large Language Models. [pdf](https://arxiv.org/pdf/2304.11082.pdf) arXiv, 2023.

- DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. [pdf](https://arxiv.org/pdf/2306.11698.pdf) arXiv, 2023.

- Large Language Model Alignment: A Survey. [pdf](https://arxiv.org/pdf/2309.15025.pdf) arXiv, 2023.

- The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risk. [pdf](https://arxiv.org/pdf/2310.15469.pdf) arXiv, 2023.

- Identifying and Mitigating the Security Risks of Generative AI. [pdf](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2308.14840v3.pdf) arXiv, 2023.

- The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning. [pdf](https://arxiv.org/pdf/2312.01552.pdf) arXiv, 2023.

- Chain-of-Verification Reduces Hallucination in Large Language Models. [pdf](https://arxiv.org/abs/2309.11495) arXiv, 2023.

- Language Is Not All You Need: Aligning Perception with Language Models. [pdf](https://arxiv.org/pdf/2302.14045.pdf) arXiv, 2023.

### Jailbreak

- GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher. [pdf](https://arxiv.org/pdf/2308.06463.pdf) arXiv, 2023.

- Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models. [pdf](http://arxiv.org/pdf/2310.02949.pdf) arXiv, 2023.

- Visual Adversarial Examples Jailbreak Aligned Large Language Models. [pdf](https://arxiv.org/abs/2306.13213) arXiv, 2023.

- Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! [pdf](https://arxiv.org/abs/2310.03693) [website](https://llm-tuning-safety.github.io/) arXiv, 2023.

- JAILBREAKER: Automated Jailbreak Across Multiple Large Language Model Chatbots. [pdf](https://arxiv.org/pdf/2307.08715.pdf) NDSS, 2024.

- Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study. [pdf](https://arxiv.org/pdf/2305.13860.pdf) arXiv, 2023.

- Multi-step Jailbreaking Privacy Attacks on ChatGPT. [pdf](https://arxiv.org/pdf/2304.05197.pdf) arXiv, 2023.

- Jailbroken: How Does LLM Safety Training Fail? [pdf](https://arxiv.org/pdf/2307.02483.pdf) arXiv, 2023.

- [workshop] On the Privacy Risk of In-context Learning. [pdf](https://trustnlpworkshop.github.io/papers/13.pdf) arXiv, 2023.

- Jailbreaking Black Box Large Language Models in Twenty Queries. [pdf](https://arxiv.org/pdf/2310.08419.pdf) arXiv, 2023.

- Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation. [pdf](https://arxiv.org/pdf/2311.03348.pdf) arXiv, 2023.

- Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models. [pdf](https://arxiv.org/pdf/2307.08487.pdf) arXiv, 2023.

- AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models. [pdf](https://arxiv.org/pdf/2310.15140.pdf) arXiv, 2023.

- "Open Sesame! Universal Black Box Jailbreaking of Large Language Models. [pdf](https://arxiv.org/abs/2309.01446) arXiv, 2023.


### Others
- LAMBRETTA: Learning to Rank for Twitter Soft Moderation. [pdf](https://arxiv.org/pdf/2212.05926.pdf) S&P, 2023.

- SoK: Content Moderation in Social Media, from Guidelines to Enforcement, and Research to Practice.[pdf](https://arxiv.org/pdf/2206.14855.pdf) arXiv, 2023.

- You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content. [pdf](https://yangzhangalmo.github.io/papers/SP24-ToxicPrompt.pdf) S&P, 2024.

- Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection. [pdf](https://aclanthology.org/2023.acl-long.22.pdf) ACL 2023.

- Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning. [pdf](https://arxiv.org/pdf/2310.11397.pdf) arXiv, 2023.

- Is ChatGPT a General-Purpose Natural Language Processing Task Solver? [pdf](https://arxiv.org/abs/2302.06476) arXiv, 2023.

- Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models. [pdf](https://arxiv.org/pdf/2311.00871.pdf) arXiv, 2023.

- [website] Jailbreaking Large Language Models: Techniques, Examples, Prevention Methods [link](https://www.lakera.ai/blog/jailbreaking-large-language-models-guide)

- Text Embeddings Reveal (Almost) As Much As Text. [pdf](https://aclanthology.org/2023.emnlp-main.765/) EMNLP, 2023.
